{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fytroo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import serializers\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no argsparse\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict({\n",
    "    'bs': 64, \n",
    "    'epoch' : 100,\n",
    "    'lr' : 0.05,\n",
    "    'gpu': 0,\n",
    "    'out': 'result',\n",
    "    'resume': '',\n",
    "    'n_in': 32, \n",
    "})\n",
    "try:\n",
    "    __file__.endswith('py')\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Chainer example: MNIST')\n",
    "    parser.add_argument('--batchsize', '-b', dest='bs', type=int, default=args.bs,\n",
    "                        help='Number of images in each mini-batch')\n",
    "    parser.add_argument('--epoch', '-e', type=int, default=args.epoch,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--learningrate', '-l', dest='lr', type=float, default=args.lr,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--frequency', '-f', type=int, default=-1,\n",
    "                        help='Frequency of taking a snapshot')\n",
    "    parser.add_argument('--gpu', '-g', type=int, default=args.gpu,\n",
    "                        help='GPU ID (negative value indicates CPU)')\n",
    "    parser.add_argument('--out', '-o', default=args.out,\n",
    "                        help='Directory to output the result')\n",
    "    parser.add_argument('--resume', '-r', default=args.resume,\n",
    "                        help='Resume the training from snapshot')\n",
    "    parser.add_argument('--unit', '-u', dest='n_in', type=int, default=args.n_in,\n",
    "                        help='Number of units')\n",
    "    parser.add_argument('--noplot', dest='plot', action='store_false',\n",
    "                        help='Disable PlotReport extension')\n",
    "    args = parser.parse_args()\n",
    "except:\n",
    "    print('no argsparse')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer.datasets import get_cifar10\n",
    "\n",
    "data_train, data_test = get_cifar10()\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(data_train.__len__()):\n",
    "    x, y = data_train.__getitem__(i)\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(data_test.__len__()):\n",
    "    x, y = data_test.__getitem__(i)\n",
    "    x_test.append(x)\n",
    "    y_test.append(y)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = np.swapaxes(x_train*255, 1, 3).astype(np.uint8)\n",
    "x_test = np.swapaxes(x_test*255, 1, 3).astype(np.uint8)\n",
    "\n",
    "x_train.shape, x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Block(chainer.Chain):\n",
    "    def __init__(self, n_in=32, n_out=32, width=16, depth=1):\n",
    "        super(Block, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.depth = depth\n",
    "        ch = width\n",
    "        with self.init_scope():\n",
    "            self.bn1 = L.BatchNormalization(n_in)\n",
    "            self.conv1 = L.Convolution2D(None, ch, 1, 1, 0)\n",
    "            self.bn2 = L.BatchNormalization(ch)\n",
    "            self.conv2 = L.Convolution2D(None, ch, 1, 1, 0)\n",
    "            self.bn3 = L.BatchNormalization(ch)\n",
    "            self.conv3 = L.Convolution2D(None, n_out, 1, 1, 0)\n",
    "            self.conv = L.Convolution2D(n_in, n_out, 1, 1, 0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = x_ = x\n",
    "        h = self.bn1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn3(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv3(h)\n",
    "        \n",
    "        if self.n_out != self.n_in: \n",
    "            x_ = self.conv(x_) \n",
    "        x = h + x_\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-ad79fdb8cb0d>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-ad79fdb8cb0d>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    exec('self.block{id} = Block({n_in}, {n_out}, {width}, {depth})'.format(id=i, n_in=))\u001b[0m\n\u001b[0m                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Fire(chainer.Chain):\n",
    "    def __init__(self, n_in=None, width=32, depth=1):\n",
    "        super(Fire, self).__init__()\n",
    "        self.depth = depth\n",
    "        with self.init_scope():\n",
    "            for i in range(self.depth):\n",
    "                exec('self.block{id} = Block({n_in}, {n_out}, {width}, {depth})'.format(id=i, n_in=))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for i in range(self.depth):\n",
    "            h = x\n",
    "            h = eval('self.bn{}'.format(i))(h)\n",
    "            h = F.relu(h)\n",
    "            h = eval('self.conv{}'.format(i))(h)\n",
    "            #x = h + x\n",
    "            x = h\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LeNet(chainer.Chain):\n",
    "    def __init__(self, n_in=32, n_out=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 32, 3)\n",
    "            self.block1 = Block(32, 16, width=32, depth=2)\n",
    "            self.block2 = Block(16, 8, width=64, depth=2)\n",
    "            self.fc = L.Linear(None, n_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        #x = F.dropout(x, 0.25)\n",
    "        x = self.block1(x)\n",
    "        x = F.max_pooling_2d(x, ksize=2, stride=2)\n",
    "        #x = F.dropout(x, 0.25)\n",
    "        x = self.block2(x)\n",
    "        #x = F.dropout(x, 0.25)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        x = self.fc(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_in = x_train.shape[-1]\n",
    "n_label = np.unique(y_train).size\n",
    "model = L.Classifier(LeNet(n_in, n_label),\n",
    "                    lossfun=F.softmax_cross_entropy,\n",
    "                    accfun=F.accuracy)\n",
    "xp = np\n",
    "if args.gpu >= 0:\n",
    "    import cupy as cp\n",
    "    xp = cp\n",
    "    chainer.cuda.get_device_from_id(args.gpu).use()\n",
    "    model.to_gpu()  # Copy the model to the GPU\n",
    "optimizer = chainer.optimizers.MomentumSGD(args.lr)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "import Augmentor\n",
    "p = Augmentor.Pipeline()\n",
    "p.flip_left_right(probability=0.5)\n",
    "p.flip_top_bottom(probability=0.5)\n",
    "g = p.keras_generator_from_array(x_train, y_train, batch_size=args.bs)\n",
    "g = ((\n",
    "    xp.array(np.swapaxes((x/255.), 1, 3)).astype(np.float32),\n",
    "    xp.array(y.astype(np.int8))\n",
    "    ) for (x,y) in g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chainer.trainingを使わず，訓練ループをかく\n",
    "chainer.trainingでは，自前のデータのイテレータを使うことができないため．\n",
    "Augmentorを使いたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練と検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(step=None):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    n_data = 0\n",
    "    n_train = len(y_train)\n",
    "    for _ in range(n_train//args.bs):\n",
    "        xs, ts = next(g) \n",
    "        x = chainer.Variable(xs)\n",
    "        t = chainer.Variable(ts)\n",
    "        optimizer.update(model, x, t)\n",
    "        with chainer.using_config('train', True):\n",
    "            loss = model(x,t)\n",
    "        n_data += len(t.data)\n",
    "        total_loss += float(loss.data) * len(t.data)\n",
    "        total_acc += float(model.accuracy.data) * len(t.data)\n",
    "\n",
    "    loss = total_loss / n_data\n",
    "    acc = total_acc / n_data\n",
    "    print('loss: {:.4f}\\t acc: {:.4f}'.format(loss, acc))\n",
    "\n",
    "def test(step=None):\n",
    "    xs = xp.array(np.swapaxes((x_test), 1, 3)).astype(np.float32)\n",
    "    ts = xp.array(y_test).astype(np.int8)\n",
    "    x = chainer.Variable(xs)\n",
    "    t = chainer.Variable(ts)\n",
    "    loss = model(x,t)\n",
    "\n",
    "    n_data = len(t.data)\n",
    "    total_loss = float(loss.data) * len(t.data)\n",
    "    total_acc = float(model.accuracy.data) * len(t.data)\n",
    "    loss = total_loss / n_data\n",
    "    acc = total_acc / n_data\n",
    "    print('val_loss: {:.4f}\\t val_acc: {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss: nan\t acc: 0.1033\n",
      "val_loss: nan\t val_acc: 0.1000\n",
      "step:1\n",
      "loss: nan\t acc: 0.1001\n",
      "val_loss: nan\t val_acc: 0.1000\n",
      "step:2\n",
      "loss: nan\t acc: 0.1002\n",
      "val_loss: nan\t val_acc: 0.1000\n",
      "step:3\n",
      "loss: nan\t acc: 0.0993\n",
      "val_loss: nan\t val_acc: 0.1000\n",
      "step:4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for step in range(args.epoch):\n",
    "        print('step:{}'.format(step))\n",
    "        train(step)\n",
    "        test(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
