{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fytroo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import chainer\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "import chainer.initializers as I\n",
    "from chainer import serializers\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no argsparse\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict\n",
    "args = EasyDict({\n",
    "    'bs': 32, \n",
    "    'epoch' : 100,\n",
    "    'lr' : 0.005,\n",
    "    'gpu': 0,\n",
    "    'out': 'result',\n",
    "    'resume': '',\n",
    "})\n",
    "try:\n",
    "    __file__.endswith('py')\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='Chainer example: MNIST')\n",
    "    parser.add_argument('--batchsize', '-b', dest='bs', type=int, default=args.bs,\n",
    "                        help='Number of images in each mini-batch')\n",
    "    parser.add_argument('--epoch', '-e', type=int, default=args.epoch,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--learningrate', '-l', dest='lr', type=float, default=args.lr,\n",
    "                        help='Number of sweeps over the dataset to train')\n",
    "    parser.add_argument('--frequency', '-f', type=int, default=-1,\n",
    "                        help='Frequency of taking a snapshot')\n",
    "    parser.add_argument('--gpu', '-g', type=int, default=args.gpu,\n",
    "                        help='GPU ID (negative value indicates CPU)')\n",
    "    parser.add_argument('--out', '-o', default=args.out,\n",
    "                        help='Directory to output the result')\n",
    "    parser.add_argument('--resume', '-r', default=args.resume,\n",
    "                        help='Resume the training from snapshot')\n",
    "    parser.add_argument('--unit', '-u', dest='n_in', type=int, default=args.n_in,\n",
    "                        help='Number of units')\n",
    "    parser.add_argument('--noplot', dest='plot', action='store_false',\n",
    "                        help='Disable PlotReport extension')\n",
    "    args = parser.parse_args()\n",
    "except:\n",
    "    print('no argsparse')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), dtype('uint8'), dtype('uint8'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from chainer.datasets import get_cifar10\n",
    "\n",
    "data_train, data_test = get_cifar10()\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(data_train.__len__()):\n",
    "    x, y = data_train.__getitem__(i)\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in range(data_test.__len__()):\n",
    "    x, y = data_test.__getitem__(i)\n",
    "    x_test.append(x)\n",
    "    y_test.append(y)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train = np.swapaxes(x_train*255, 1, 3).astype(np.uint8)\n",
    "x_test = np.swapaxes(x_test*255, 1, 3).astype(np.uint8)\n",
    "\n",
    "x_train.shape, x_train.dtype, x_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Block(chainer.Chain):\n",
    "    def __init__(self, n_in=32, ch=16):\n",
    "        super(Block, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.ch = ch\n",
    "        \n",
    "        initialW= I.HeNormal()\n",
    "        with self.init_scope():\n",
    "            self.bn1 = L.BatchNormalization(n_in)\n",
    "            self.conv1 = L.Convolution2D(None, ch, 3, 1, 1, initialW=initialW, nobias=True)\n",
    "            self.bn2 = L.BatchNormalization(ch)\n",
    "            self.conv2 = L.Convolution2D(None, ch, 3, 1, 1, initialW=initialW, nobias=True)\n",
    "            self.bn3 = L.BatchNormalization(ch)\n",
    "            \n",
    "            self.conv = L.Convolution2D(None, ch, 1, 1, 0)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = x_ = x\n",
    "        h = self.bn1(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn2(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        h = self.bn3(h)\n",
    "        \n",
    "        if self.n_in != self.ch:\n",
    "            x_ = self.conv(x_)\n",
    "        \n",
    "        return h + x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Section(chainer.ChainList):\n",
    "    def __init__(self, n_in=32, ch=32, depth=1):\n",
    "        super(Section, self).__init__()\n",
    "        self.add_link(Block(n_in, ch))\n",
    "        for i in range(1, depth):\n",
    "            self.add_link(Block(ch, ch))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for f in self.children():\n",
    "            x = f(x)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_in=32, n_out=10):\n",
    "        super(CNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 16, 3)\n",
    "            self.section1 = Section(16, 16, 2)\n",
    "            self.section2 = Section(16, 32, 2)\n",
    "            self.section3 = Section(32, 64, 2)\n",
    "            self.conv_out = L.Convolution2D(None, n_out, 3)\n",
    "            self.fc = L.Linear(None, n_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.section1(x)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        x = self.section2(x)\n",
    "        #print('#2', x.shape)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        x = self.section3(x)\n",
    "        x = self.conv_out(x)\n",
    "        x = F.squeeze(F.average_pooling_2d(x, 5))\n",
    "        #print('#2', x.shape)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    def __init__(self, n_in=32, n_out=10):\n",
    "        super(CNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv1 = L.Convolution2D(None, 16, 3)\n",
    "            self.section1 = Section(16, 16, 2)\n",
    "            self.section2 = Section(16, 32, 2)\n",
    "            self.section3 = Section(32, 64, 2)\n",
    "            self.fc = L.Linear(None, n_out)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.section1(x)\n",
    "        #print('#1', x.shape)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        x = self.section2(x)\n",
    "        #print('#2', x.shape)\n",
    "        x = F.average_pooling_2d(x, ksize=2, stride=2)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_label = np.unique(y_train).size\n",
    "model = L.Classifier(CNN(n_label),\n",
    "                    lossfun=F.softmax_cross_entropy,\n",
    "                    accfun=F.accuracy)\n",
    "xp = np\n",
    "if args.gpu >= 0:\n",
    "    import cupy as cp\n",
    "    xp = cp\n",
    "    chainer.cuda.get_device_from_id(args.gpu).use()\n",
    "    model.to_gpu()  # Copy the model to the GPU\n",
    "optimizer = chainer.optimizers.MomentumSGD(args.lr)\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "import Augmentor\n",
    "p = Augmentor.Pipeline()\n",
    "p.crop_random(probability=1, percentage_area=0.8)\n",
    "p.resize(probability=1, width=32, height=32)\n",
    "p.flip_left_right(probability=0.5)\n",
    "p.random_erasing(probability=0.5, rectangle_area=0.2)\n",
    "p.shear(probability=0.3, max_shear_left=2, max_shear_right=2)\n",
    "\n",
    "g = p.keras_generator_from_array(x_train, y_train, batch_size=args.bs)\n",
    "g = ((\n",
    "    xp.array(np.swapaxes((x/255.), 1, 3)).astype(np.float32),\n",
    "    xp.array(y.astype(np.int8))\n",
    "    ) for (x,y) in g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chainer.trainingを使わず，訓練ループをかく\n",
    "chainer.trainingでは，自前のデータのイテレータを使うことができないため．\n",
    "Augmentorを使いたい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練と検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(step=None):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    n_data = 0\n",
    "    n_train = len(y_train)\n",
    "    for _ in range(n_train//args.bs):\n",
    "        xs, ts = next(g) \n",
    "        x = chainer.Variable(xs)\n",
    "        t = chainer.Variable(ts)\n",
    "        optimizer.update(model, x, t)\n",
    "        with chainer.using_config('train', True):\n",
    "            loss = model(x,t)\n",
    "        n_data += len(t.data)\n",
    "        total_loss += float(loss.data) * len(t.data)\n",
    "        total_acc += float(model.accuracy.data) * len(t.data)\n",
    "\n",
    "    loss = total_loss / n_data\n",
    "    acc = total_acc / n_data\n",
    "    print('loss: {:.4f}\\t acc: {:.4f}'.format(loss, acc))\n",
    "\n",
    "def test(step=None):\n",
    "    xs = xp.array(np.swapaxes((x_test), 1, 3)).astype(np.float32)\n",
    "    ts = xp.array(y_test).astype(np.int8)\n",
    "    x = chainer.Variable(xs)\n",
    "    t = chainer.Variable(ts)\n",
    "    loss = model(x,t)\n",
    "\n",
    "    n_data = len(t.data)\n",
    "    total_loss = float(loss.data) * len(t.data)\n",
    "    total_acc = float(model.accuracy.data) * len(t.data)\n",
    "    loss = total_loss / n_data\n",
    "    acc = total_acc / n_data\n",
    "    print('val_loss: {:.4f}\\t val_acc: {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:0\n",
      "loss: 1.5911\t acc: 0.4303\n",
      "val_loss: 115.6856\t val_acc: 0.1591\n",
      "step:1\n",
      "loss: 1.3740\t acc: 0.5053\n",
      "val_loss: 117.1912\t val_acc: 0.1617\n",
      "step:2\n",
      "loss: 1.2877\t acc: 0.5412\n",
      "val_loss: 151.6403\t val_acc: 0.1293\n",
      "step:3\n",
      "loss: 1.2102\t acc: 0.5708\n",
      "val_loss: 107.8154\t val_acc: 0.1668\n",
      "step:4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-363cff855476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'step:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d85dfc6b42a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(step)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musing_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mn_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for step in range(args.epoch):\n",
    "        print('step:{}'.format(step))\n",
    "        train(step)\n",
    "        test(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
